{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca97d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9039f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n",
      "ğŸ“Š Pandas version: 2.3.1\n",
      "ï¿½ï¿½ Matplotlib version: 3.10.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ğŸ“Š Pandas version: {pd.__version__}\")\n",
    "print(f\"ï¿½ï¿½ Matplotlib version: {plt.matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bd1c0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_data(file_path, sample_size=None):\n",
    "    \"\"\"Load CSV data with optional sampling\"\"\"\n",
    "    try:\n",
    "        if file_path.endswith('.gz'):\n",
    "            df = pd.read_csv(file_path, compression='gzip')\n",
    "        else:\n",
    "            df = pd.read_csv(file_path)\n",
    "        \n",
    "        if sample_size and len(df) > sample_size:\n",
    "            df = df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        print(f\"âœ… Loaded {file_path}\")\n",
    "        print(f\"   ğŸ“ Shape: {df.shape}\")\n",
    "        print(f\"   ï¿½ï¿½ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def load_geojson_data(file_path):\n",
    "    \"\"\"Load GeoJSON data\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            geojson_data = json.load(f)\n",
    "        \n",
    "        print(f\"âœ… Loaded {file_path}\")\n",
    "        print(f\"   ğŸ“ Features: {len(geojson_data.get('features', []))}\")\n",
    "        \n",
    "        return geojson_data\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def display_data_info(df, name):\n",
    "    \"\"\"Display comprehensive information about a dataset\"\"\"\n",
    "    print(f\"\\nğŸ“Š {name} Dataset Overview:\")\n",
    "    print(f\"   ğŸ“ Shape: {df.shape}\")\n",
    "    print(f\"   ğŸ“‹ Columns: {list(df.columns)}\")\n",
    "    print(f\"   ï¿½ï¿½ Memory: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   â“ Missing values: {df.isnull().sum().sum()}\")\n",
    "    print(f\"   ï¿½ï¿½ Duplicates: {df.duplicated().sum()}\")\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\nğŸ“ Data Types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Show first few rows\n",
    "    print(f\"\\nğŸ‘€ First 3 rows:\")\n",
    "    print(df.head(3))\n",
    "    print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fa057c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Loading Airbnb datasets...\n",
      "\n",
      "âœ… Loaded listings.csv\n",
      "   ğŸ“ Shape: (21722, 18)\n",
      "   ï¿½ï¿½ Memory: 8.92 MB\n",
      "âœ… Loaded reviews.csv\n",
      "   ğŸ“ Shape: (405687, 2)\n",
      "   ï¿½ï¿½ Memory: 25.92 MB\n",
      "âœ… Loaded calendar.csv.gz\n",
      "   ğŸ“ Shape: (7928517, 7)\n",
      "   ï¿½ï¿½ Memory: 1497.87 MB\n",
      "âœ… Loaded neighbourhoods.csv\n",
      "   ğŸ“ Shape: (11, 2)\n",
      "   ï¿½ï¿½ Memory: 0.00 MB\n",
      "âœ… Loaded neighbourhoods.geojson\n",
      "   ğŸ“ Features: 11\n",
      "\n",
      "âœ… All data files loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸš€ Loading Airbnb datasets...\\n\")\n",
    "\n",
    "# Load listings data (prefer uncompressed)\n",
    "if Path('listings.csv').exists():\n",
    "    listings_df = load_csv_data('listings.csv')\n",
    "else:\n",
    "    listings_df = load_csv_data('listings.csv.gz')\n",
    "\n",
    "# Load reviews data (prefer uncompressed)\n",
    "if Path('reviews.csv').exists():\n",
    "    reviews_df = load_csv_data('reviews.csv')\n",
    "else:\n",
    "    reviews_df = load_csv_data('reviews.csv.gz')\n",
    "\n",
    "# Load calendar data\n",
    "calendar_df = load_csv_data('calendar.csv.gz')\n",
    "\n",
    "# Load neighbourhoods data\n",
    "neighbourhoods_df = load_csv_data('neighbourhoods.csv')\n",
    "\n",
    "# Load GeoJSON data\n",
    "neighbourhoods_geojson = load_geojson_data('neighbourhoods.geojson')\n",
    "\n",
    "print(\"\\nâœ… All data files loaded!\")\n",
    "\n",
    "# Store all dataframes in a dictionary for easy access\n",
    "datasets = {\n",
    "    'Listings': listings_df,\n",
    "    'Reviews': reviews_df,\n",
    "    'Calendar': calendar_df,\n",
    "    'Neighbourhoods': neighbourhoods_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "512ba527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½ï¿½ COMPREHENSIVE DATA OVERVIEW\n",
      "\n",
      "\n",
      "ğŸ“Š Listings Dataset Overview:\n",
      "   ğŸ“ Shape: (21722, 18)\n",
      "   ğŸ“‹ Columns: ['id', 'name', 'host_id', 'host_name', 'neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'room_type', 'price', 'minimum_nights', 'number_of_reviews', 'last_review', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365', 'number_of_reviews_ltm', 'license']\n",
      "   ï¿½ï¿½ Memory: 8.92 MB\n",
      "   â“ Missing values: 58220\n",
      "   ï¿½ï¿½ Duplicates: 0\n",
      "\n",
      "ğŸ“ Data Types:\n",
      "int64      7\n",
      "float64    6\n",
      "object     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ‘€ First 3 rows:\n",
      "      id                                               name  host_id  \\\n",
      "0  31094  Beautiful, spacious, central, renovated Penthouse   129976   \n",
      "1  32379  155 m2 artist flat on Vesterbro, with 2 bathrooms   140105   \n",
      "2  32841                Cozy flat for Adults/Quiet for kids   142143   \n",
      "\n",
      "        host_name  neighbourhood_group              neighbourhood   latitude  \\\n",
      "0            Ebbe                  NaN  Vesterbro-Kongens Enghave  55.666602   \n",
      "1            Lise                  NaN  Vesterbro-Kongens Enghave  55.672638   \n",
      "2  Anders & Maria                  NaN                    sterbro  55.711760   \n",
      "\n",
      "   longitude        room_type  price  minimum_nights  number_of_reviews  \\\n",
      "0  12.555283  Entire home/apt    NaN               3                 19   \n",
      "1  12.552493  Entire home/apt    NaN               3                 84   \n",
      "2  12.570910  Entire home/apt    NaN             100                  7   \n",
      "\n",
      "  last_review  reviews_per_month  calculated_host_listings_count  \\\n",
      "0  2022-08-22               0.11                               1   \n",
      "1  2024-10-28               0.47                               2   \n",
      "2  2016-09-15               0.04                               1   \n",
      "\n",
      "   availability_365  number_of_reviews_ltm  license  \n",
      "0                 0                      0      NaN  \n",
      "1                 0                      3      NaN  \n",
      "2                15                      0      NaN  \n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Reviews Dataset Overview:\n",
      "   ğŸ“ Shape: (405687, 2)\n",
      "   ğŸ“‹ Columns: ['listing_id', 'date']\n",
      "   ï¿½ï¿½ Memory: 25.92 MB\n",
      "   â“ Missing values: 0\n",
      "   ï¿½ï¿½ Duplicates: 5549\n",
      "\n",
      "ğŸ“ Data Types:\n",
      "int64     1\n",
      "object    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ‘€ First 3 rows:\n",
      "   listing_id        date\n",
      "0       31094  2010-08-16\n",
      "1       31094  2011-01-05\n",
      "2       31094  2012-06-10\n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Calendar Dataset Overview:\n",
      "   ğŸ“ Shape: (7928517, 7)\n",
      "   ğŸ“‹ Columns: ['listing_id', 'date', 'available', 'price', 'adjusted_price', 'minimum_nights', 'maximum_nights']\n",
      "   ï¿½ï¿½ Memory: 1497.87 MB\n",
      "   â“ Missing values: 7929319\n",
      "   ï¿½ï¿½ Duplicates: 0\n",
      "\n",
      "ğŸ“ Data Types:\n",
      "object     3\n",
      "float64    3\n",
      "int64      1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ‘€ First 3 rows:\n",
      "   listing_id        date available      price  adjusted_price  \\\n",
      "0       31094  2025-03-24         f  $2,299.00             NaN   \n",
      "1       31094  2025-03-25         f  $2,299.00             NaN   \n",
      "2       31094  2025-03-26         f  $2,299.00             NaN   \n",
      "\n",
      "   minimum_nights  maximum_nights  \n",
      "0             3.0            10.0  \n",
      "1             3.0            10.0  \n",
      "2             3.0            10.0  \n",
      "\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Neighbourhoods Dataset Overview:\n",
      "   ğŸ“ Shape: (11, 2)\n",
      "   ğŸ“‹ Columns: ['neighbourhood_group', 'neighbourhood']\n",
      "   ï¿½ï¿½ Memory: 0.00 MB\n",
      "   â“ Missing values: 11\n",
      "   ï¿½ï¿½ Duplicates: 0\n",
      "\n",
      "ğŸ“ Data Types:\n",
      "float64    1\n",
      "object     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ğŸ‘€ First 3 rows:\n",
      "   neighbourhood_group neighbourhood\n",
      "0                  NaN     Amager st\n",
      "1                  NaN   Amager Vest\n",
      "2                  NaN    Bispebjerg\n",
      "\n",
      "============================================================\n",
      "\n",
      "ï¿½ï¿½ Summary:\n",
      "   âœ… Successfully loaded: 4/4 datasets\n",
      "   ğŸ“ GeoJSON: âœ…\n"
     ]
    }
   ],
   "source": [
    "print(\"ï¿½ï¿½ COMPREHENSIVE DATA OVERVIEW\\n\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        display_data_info(df, name)\n",
    "    else:\n",
    "        print(f\"âŒ {name} Dataset: Failed to load\\n\")\n",
    "\n",
    "print(\"\\nï¿½ï¿½ Summary:\")\n",
    "successful_loads = sum(1 for df in datasets.values() if df is not None)\n",
    "total_datasets = len(datasets)\n",
    "print(f\"   âœ… Successfully loaded: {successful_loads}/{total_datasets} datasets\")\n",
    "print(f\"   ğŸ“ GeoJSON: {'âœ…' if neighbourhoods_geojson is not None else 'âŒ'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3cb58f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ  LISTINGS DATA ANALYSIS\n",
      "\n",
      "ğŸ“ˆ Numeric Columns Statistics:\n",
      "                 id       host_id  neighbourhood_group      latitude  \\\n",
      "count  2.172200e+04  2.172200e+04                  0.0  21722.000000   \n",
      "mean   5.991839e+17  1.543218e+08                  NaN     55.680526   \n",
      "std    5.172072e+17  1.800592e+08                  NaN      0.019103   \n",
      "min    3.109400e+04  5.130000e+02                  NaN     55.615660   \n",
      "25%    3.483605e+07  1.834102e+07                  NaN     55.666275   \n",
      "50%    7.038996e+17  6.907512e+07                  NaN     55.681850   \n",
      "75%    1.089093e+18  2.330120e+08                  NaN     55.695940   \n",
      "max    1.382678e+18  6.851944e+08                  NaN     55.732470   \n",
      "\n",
      "          longitude          price  minimum_nights  number_of_reviews  \\\n",
      "count  21722.000000   12879.000000    21722.000000       21722.000000   \n",
      "mean      12.558699    1291.329296        4.613341          18.676319   \n",
      "std        0.031187    1280.272641       17.624078          44.574748   \n",
      "min       12.453748     136.000000        1.000000           0.000000   \n",
      "25%       12.540844     840.000000        2.000000           2.000000   \n",
      "50%       12.555170    1100.000000        3.000000           7.000000   \n",
      "75%       12.580650    1500.000000        4.000000          19.000000   \n",
      "max       12.639720  100000.000000     1111.000000        2277.000000   \n",
      "\n",
      "       reviews_per_month  calculated_host_listings_count  availability_365  \\\n",
      "count       18758.000000                    21722.000000      21722.000000   \n",
      "mean            0.637023                        4.186447         80.215266   \n",
      "std             1.313397                       22.588550        113.206850   \n",
      "min             0.010000                        1.000000          0.000000   \n",
      "25%             0.180000                        1.000000          0.000000   \n",
      "50%             0.360000                        1.000000         15.000000   \n",
      "75%             0.710000                        1.000000        126.000000   \n",
      "max            84.840000                      215.000000        365.000000   \n",
      "\n",
      "       number_of_reviews_ltm  license  \n",
      "count           21722.000000      0.0  \n",
      "mean                5.120753      NaN  \n",
      "std                12.900415      NaN  \n",
      "min                 0.000000      NaN  \n",
      "25%                 0.000000      NaN  \n",
      "50%                 2.000000      NaN  \n",
      "75%                 6.000000      NaN  \n",
      "max               773.000000      NaN  \n",
      "\n",
      "ğŸ“‹ Categorical Columns (5):\n",
      "   name: 19522 unique values\n",
      "   host_name: 5743 unique values\n",
      "   neighbourhood: 11 unique values\n",
      "      Values: ['Vesterbro-Kongens Enghave', 'sterbro', 'Indre By', 'Amager Vest', 'Nrrebro', 'Frederiksberg', 'Bispebjerg', 'Amager st', 'Valby', 'Vanlse', 'Brnshj-Husum']\n",
      "   room_type: 4 unique values\n",
      "      Values: ['Entire home/apt', 'Private room', 'Shared room', 'Hotel room']\n",
      "   last_review: 1689 unique values\n",
      "\n",
      "ğŸ’° Price-related columns: ['price']\n",
      "   price: count     12879.000000\n",
      "mean       1291.329296\n",
      "std        1280.272641\n",
      "min         136.000000\n",
      "25%         840.000000\n",
      "50%        1100.000000\n",
      "75%        1500.000000\n",
      "max      100000.000000\n",
      "Name: price, dtype: float64\n",
      "\n",
      "ğŸ“ Location-related columns: ['neighbourhood_group', 'neighbourhood', 'latitude', 'longitude', 'calculated_host_listings_count']\n"
     ]
    }
   ],
   "source": [
    "if listings_df is not None:\n",
    "    print(\"ğŸ  LISTINGS DATA ANALYSIS\\n\")\n",
    "    \n",
    "    # Basic statistics for numeric columns\n",
    "    numeric_cols = listings_df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(\"ğŸ“ˆ Numeric Columns Statistics:\")\n",
    "        print(listings_df[numeric_cols].describe())\n",
    "    \n",
    "    # Analyze categorical columns\n",
    "    categorical_cols = listings_df.select_dtypes(include=['object']).columns\n",
    "    print(f\"\\nğŸ“‹ Categorical Columns ({len(categorical_cols)}):\")\n",
    "    for col in categorical_cols[:10]:  # Show first 10\n",
    "        unique_count = listings_df[col].nunique()\n",
    "        print(f\"   {col}: {unique_count} unique values\")\n",
    "        if unique_count <= 15:  # Show values if not too many\n",
    "            print(f\"      Values: {list(listings_df[col].unique())}\")\n",
    "    \n",
    "    # Check for price-related columns\n",
    "    price_cols = [col for col in listings_df.columns if 'price' in col.lower()]\n",
    "    if price_cols:\n",
    "        print(f\"\\nğŸ’° Price-related columns: {price_cols}\")\n",
    "        for col in price_cols:\n",
    "            if col in listings_df.columns:\n",
    "                print(f\"   {col}: {listings_df[col].describe()}\")\n",
    "    \n",
    "    # Check for location columns\n",
    "    location_cols = [col for col in listings_df.columns if any(x in col.lower() for x in ['lat', 'lon', 'neighbourhood', 'city'])]\n",
    "    if location_cols:\n",
    "        print(f\"\\nğŸ“ Location-related columns: {location_cols}\")\n",
    "else:\n",
    "    print(\"âŒ Listings data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33339d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â­ REVIEWS DATA ANALYSIS\n",
      "\n",
      "ï¿½ï¿½ Reviews Statistics:\n",
      "         listing_id\n",
      "count  4.056870e+05\n",
      "mean   2.612276e+17\n",
      "std    4.168974e+17\n",
      "min    3.109400e+04\n",
      "25%    1.209163e+07\n",
      "50%    3.296784e+07\n",
      "75%    6.453624e+17\n",
      "max    1.373734e+18\n",
      "\n",
      "ğŸ“… Date columns: ['date']\n",
      "   âœ… Converted date to datetime\n",
      "   ï¿½ï¿½ Range: 2010-07-25 00:00:00 to 2025-03-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if reviews_df is not None:\n",
    "    print(\"â­ REVIEWS DATA ANALYSIS\\n\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"ï¿½ï¿½ Reviews Statistics:\")\n",
    "    print(reviews_df.describe())\n",
    "    \n",
    "    # Check for rating columns\n",
    "    rating_cols = [col for col in reviews_df.columns if 'rating' in col.lower() or 'score' in col.lower()]\n",
    "    if rating_cols:\n",
    "        print(f\"\\nâ­ Rating columns: {rating_cols}\")\n",
    "        for col in rating_cols:\n",
    "            if col in reviews_df.columns:\n",
    "                print(f\"   {col}: {reviews_df[col].describe()}\")\n",
    "    \n",
    "    # Check for date columns\n",
    "    date_cols = [col for col in reviews_df.columns if 'date' in col.lower()]\n",
    "    if date_cols:\n",
    "        print(f\"\\nğŸ“… Date columns: {date_cols}\")\n",
    "        for col in date_cols:\n",
    "            try:\n",
    "                reviews_df[col] = pd.to_datetime(reviews_df[col])\n",
    "                print(f\"   âœ… Converted {col} to datetime\")\n",
    "                print(f\"   ï¿½ï¿½ Range: {reviews_df[col].min()} to {reviews_df[col].max()}\")\n",
    "            except:\n",
    "                print(f\"   âŒ Could not convert {col} to datetime\")\n",
    "    \n",
    "    # Show sample reviews if comments column exists\n",
    "    if 'comments' in reviews_df.columns:\n",
    "        print(f\"\\nï¿½ï¿½ Sample Reviews:\")\n",
    "        sample_reviews = reviews_df['comments'].dropna().head(2)\n",
    "        for i, review in enumerate(sample_reviews, 1):\n",
    "            preview = review[:150] + \"...\" if len(review) > 150 else review\n",
    "            print(f\"   Review {i}: {preview}\")\n",
    "else:\n",
    "    print(\"âŒ Reviews data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d98bf30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“… CALENDAR DATA ANALYSIS\n",
      "\n",
      "ğŸ“Š Calendar Statistics:\n",
      "         listing_id  adjusted_price  minimum_nights  maximum_nights\n",
      "count  7.928517e+06             0.0    7.928116e+06    7.928116e+06\n",
      "mean   5.991821e+17             NaN    4.658711e+00    3.914670e+02\n",
      "std    5.171949e+17             NaN    1.699077e+01    4.343241e+02\n",
      "min    3.109400e+04             NaN    1.000000e+00    1.000000e+00\n",
      "25%    3.483549e+07             NaN    2.000000e+00    2.100000e+01\n",
      "50%    7.037323e+17             NaN    3.000000e+00    3.650000e+02\n",
      "75%    1.089095e+18             NaN    4.000000e+00    7.000000e+02\n",
      "max    1.382678e+18             NaN    1.111000e+03    1.125000e+03\n",
      "\n",
      "ğŸ“… Date columns: ['date']\n",
      "   âœ… Converted date to datetime\n",
      "   ğŸ“… Range: 2025-03-23 00:00:00 to 2026-03-27 00:00:00\n",
      "\n",
      "ğŸ’° Price columns: ['price', 'adjusted_price']\n",
      "   price: count       7928517\n",
      "unique         1625\n",
      "top       $1,000.00\n",
      "freq         501516\n",
      "Name: price, dtype: object\n",
      "   adjusted_price: count    0.0\n",
      "mean     NaN\n",
      "std      NaN\n",
      "min      NaN\n",
      "25%      NaN\n",
      "50%      NaN\n",
      "75%      NaN\n",
      "max      NaN\n",
      "Name: adjusted_price, dtype: float64\n",
      "\n",
      "âœ… Availability columns: ['available']\n",
      "   available: available\n",
      "f    6143151\n",
      "t    1785366\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if calendar_df is not None:\n",
    "    print(\"ğŸ“… CALENDAR DATA ANALYSIS\\n\")\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"ğŸ“Š Calendar Statistics:\")\n",
    "    print(calendar_df.describe())\n",
    "    \n",
    "    # Check for date columns\n",
    "    date_cols = [col for col in calendar_df.columns if 'date' in col.lower()]\n",
    "    if date_cols:\n",
    "        print(f\"\\nğŸ“… Date columns: {date_cols}\")\n",
    "        for col in date_cols:\n",
    "            try:\n",
    "                calendar_df[col] = pd.to_datetime(calendar_df[col])\n",
    "                print(f\"   âœ… Converted {col} to datetime\")\n",
    "                print(f\"   ğŸ“… Range: {calendar_df[col].min()} to {calendar_df[col].max()}\")\n",
    "            except:\n",
    "                print(f\"   âŒ Could not convert {col} to datetime\")\n",
    "    \n",
    "    # Check for price columns\n",
    "    price_cols = [col for col in calendar_df.columns if 'price' in col.lower()]\n",
    "    if price_cols:\n",
    "        print(f\"\\nğŸ’° Price columns: {price_cols}\")\n",
    "        for col in price_cols:\n",
    "            if col in calendar_df.columns:\n",
    "                print(f\"   {col}: {calendar_df[col].describe()}\")\n",
    "    \n",
    "    # Check for availability columns\n",
    "    avail_cols = [col for col in calendar_df.columns if 'available' in col.lower()]\n",
    "    if avail_cols:\n",
    "        print(f\"\\nâœ… Availability columns: {avail_cols}\")\n",
    "        for col in avail_cols:\n",
    "            if col in calendar_df.columns:\n",
    "                print(f\"   {col}: {calendar_df[col].value_counts()}\")\n",
    "else:\n",
    "    print(\"âŒ Calendar data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca872b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DATA QUALITY ASSESSMENT\n",
      "\n",
      "ğŸ“Š Listings Dataset Quality:\n",
      "   â“ Missing values by column:\n",
      "      host_name: 5 (0.02%)\n",
      "      neighbourhood_group: 21722 (100.00%)\n",
      "      price: 8843 (40.71%)\n",
      "      last_review: 2964 (13.65%)\n",
      "      reviews_per_month: 2964 (13.65%)\n",
      "      license: 21722 (100.00%)\n",
      "   ğŸ”„ Duplicate rows: 0\n",
      "   ğŸ“ Data types: {dtype('int64'): 7, dtype('float64'): 6, dtype('O'): 5}\n",
      "\n",
      "ğŸ“Š Reviews Dataset Quality:\n",
      "   â“ Missing values by column:\n",
      "   ğŸ”„ Duplicate rows: 5549\n",
      "   ğŸ“ Data types: {dtype('int64'): 1, dtype('<M8[ns]'): 1}\n",
      "\n",
      "ğŸ“Š Calendar Dataset Quality:\n",
      "   â“ Missing values by column:\n",
      "      adjusted_price: 7928517 (100.00%)\n",
      "      minimum_nights: 401 (0.01%)\n",
      "      maximum_nights: 401 (0.01%)\n",
      "   ğŸ”„ Duplicate rows: 0\n",
      "   ğŸ“ Data types: {dtype('float64'): 3, dtype('O'): 2, dtype('<M8[ns]'): 1, dtype('int64'): 1}\n",
      "\n",
      "ğŸ“Š Neighbourhoods Dataset Quality:\n",
      "   â“ Missing values by column:\n",
      "      neighbourhood_group: 11 (100.00%)\n",
      "   ğŸ”„ Duplicate rows: 0\n",
      "   ğŸ“ Data types: {dtype('float64'): 1, dtype('O'): 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” DATA QUALITY ASSESSMENT\\n\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"ğŸ“Š {name} Dataset Quality:\")\n",
    "        \n",
    "        # Missing values analysis\n",
    "        missing_values = df.isnull().sum()\n",
    "        missing_percentage = (missing_values / len(df)) * 100\n",
    "        \n",
    "        print(\"   â“ Missing values by column:\")\n",
    "        for col in df.columns:\n",
    "            if missing_values[col] > 0:\n",
    "                print(f\"      {col}: {missing_values[col]} ({missing_percentage[col]:.2f}%)\")\n",
    "        \n",
    "        # Duplicate analysis\n",
    "        duplicates = df.duplicated().sum()\n",
    "        print(f\"   ğŸ”„ Duplicate rows: {duplicates}\")\n",
    "        \n",
    "        # Data types summary\n",
    "        print(f\"   ğŸ“ Data types: {df.dtypes.value_counts().to_dict()}\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"âŒ {name} Dataset: Not available for quality assessment\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cadf91b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½ï¿½ ANALYSIS SUMMARY\n",
      "\n",
      "ğŸ“Š Successfully loaded datasets:\n",
      "   âœ… Listings: (21722, 18)\n",
      "   âœ… Reviews: (405687, 2)\n",
      "   âœ… Calendar: (7928517, 7)\n",
      "   âœ… Neighbourhoods: (11, 2)\n",
      "\n",
      "ğŸ“ GeoJSON: âœ…\n",
      "\n",
      "ğŸš€ Recommended Next Steps:\n",
      "1. ğŸ”§ Data Cleaning: Handle missing values and data type conversions\n",
      "2. ï¿½ï¿½ Data Merging: Join datasets on common keys (listing_id, neighbourhood)\n",
      "3. ğŸ“Š EDA: Explore relationships between variables\n",
      "4. ğŸ“ˆ Advanced Visualizations: Create interactive plots and maps\n",
      "5. ğŸ¯ Feature Engineering: Create new features for analysis\n",
      "6. ğŸ¤– Machine Learning: Build predictive models\n",
      "7. ï¿½ï¿½ Insights: Generate actionable business insights\n",
      "\n",
      "ğŸ’¡ Key Analysis Opportunities:\n",
      "   â€¢ Price analysis and market trends\n",
      "   â€¢ Location-based insights\n",
      "   â€¢ Property type and amenity analysis\n",
      "   â€¢ Sentiment analysis of reviews\n",
      "   â€¢ Rating patterns and trends\n",
      "   â€¢ Seasonal pricing patterns\n",
      "   â€¢ Availability analysis\n",
      "\n",
      "âœ… Ready for advanced analysis! ğŸš€\n"
     ]
    }
   ],
   "source": [
    "print(\"ï¿½ï¿½ ANALYSIS SUMMARY\\n\")\n",
    "\n",
    "print(\"ğŸ“Š Successfully loaded datasets:\")\n",
    "for name, df in datasets.items():\n",
    "    status = \"âœ…\" if df is not None else \"âŒ\"\n",
    "    shape = df.shape if df is not None else \"N/A\"\n",
    "    print(f\"   {status} {name}: {shape}\")\n",
    "\n",
    "print(f\"\\nğŸ“ GeoJSON: {'âœ…' if neighbourhoods_geojson is not None else 'âŒ'}\")\n",
    "\n",
    "print(\"\\nğŸš€ Recommended Next Steps:\")\n",
    "print(\"1. ğŸ”§ Data Cleaning: Handle missing values and data type conversions\")\n",
    "print(\"2. ï¿½ï¿½ Data Merging: Join datasets on common keys (listing_id, neighbourhood)\")\n",
    "print(\"3. ğŸ“Š EDA: Explore relationships between variables\")\n",
    "print(\"4. ğŸ“ˆ Advanced Visualizations: Create interactive plots and maps\")\n",
    "print(\"5. ğŸ¯ Feature Engineering: Create new features for analysis\")\n",
    "print(\"6. ğŸ¤– Machine Learning: Build predictive models\")\n",
    "print(\"7. ï¿½ï¿½ Insights: Generate actionable business insights\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Key Analysis Opportunities:\")\n",
    "if listings_df is not None:\n",
    "    print(\"   â€¢ Price analysis and market trends\")\n",
    "    print(\"   â€¢ Location-based insights\")\n",
    "    print(\"   â€¢ Property type and amenity analysis\")\n",
    "if reviews_df is not None:\n",
    "    print(\"   â€¢ Sentiment analysis of reviews\")\n",
    "    print(\"   â€¢ Rating patterns and trends\")\n",
    "if calendar_df is not None:\n",
    "    print(\"   â€¢ Seasonal pricing patterns\")\n",
    "    print(\"   â€¢ Availability analysis\")\n",
    "\n",
    "print(\"\\nâœ… Ready for advanced analysis! ğŸš€\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75d27c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listings\n",
      "      id                                               name  host_id  \\\n",
      "0  31094  Beautiful, spacious, central, renovated Penthouse   129976   \n",
      "1  32379  155 m2 artist flat on Vesterbro, with 2 bathrooms   140105   \n",
      "2  32841                Cozy flat for Adults/Quiet for kids   142143   \n",
      "3  38499                     0 min. from everything in Cph.   122489   \n",
      "4  39055            Stylish apartment in central Copenhagen   167511   \n",
      "\n",
      "        host_name  neighbourhood_group              neighbourhood   latitude  \\\n",
      "0            Ebbe                  NaN  Vesterbro-Kongens Enghave  55.666602   \n",
      "1            Lise                  NaN  Vesterbro-Kongens Enghave  55.672638   \n",
      "2  Anders & Maria                  NaN                    sterbro  55.711760   \n",
      "3       Christina                  NaN                   Indre By  55.684288   \n",
      "4           Rikke                  NaN                Amager Vest  55.665070   \n",
      "\n",
      "   longitude        room_type   price  minimum_nights  number_of_reviews  \\\n",
      "0  12.555283  Entire home/apt     NaN               3                 19   \n",
      "1  12.552493  Entire home/apt     NaN               3                 84   \n",
      "2  12.570910  Entire home/apt     NaN             100                  7   \n",
      "3  12.573019  Entire home/apt  2550.0               7                 34   \n",
      "4  12.583150  Entire home/apt  2168.0               2                112   \n",
      "\n",
      "  last_review  reviews_per_month  calculated_host_listings_count  \\\n",
      "0  2022-08-22               0.11                               1   \n",
      "1  2024-10-28               0.47                               2   \n",
      "2  2016-09-15               0.04                               1   \n",
      "3  2024-09-21               0.19                               1   \n",
      "4  2024-12-17               0.63                               1   \n",
      "\n",
      "   availability_365  number_of_reviews_ltm  license  \n",
      "0                 0                      0      NaN  \n",
      "1                 0                      3      NaN  \n",
      "2                15                      0      NaN  \n",
      "3                52                     10      NaN  \n",
      "4               146                     20      NaN  \n",
      "Reviews\n",
      "   listing_id       date\n",
      "0       31094 2010-08-16\n",
      "1       31094 2011-01-05\n",
      "2       31094 2012-06-10\n",
      "3       31094 2013-08-24\n",
      "4       31094 2013-08-26\n",
      "Calendar\n",
      "   listing_id       date available      price  adjusted_price  minimum_nights  \\\n",
      "0       31094 2025-03-24         f  $2,299.00             NaN             3.0   \n",
      "1       31094 2025-03-25         f  $2,299.00             NaN             3.0   \n",
      "2       31094 2025-03-26         f  $2,299.00             NaN             3.0   \n",
      "3       31094 2025-03-27         f  $2,299.00             NaN             3.0   \n",
      "4       31094 2025-03-28         f  $2,299.00             NaN             3.0   \n",
      "\n",
      "   maximum_nights  \n",
      "0            10.0  \n",
      "1            10.0  \n",
      "2            10.0  \n",
      "3            10.0  \n",
      "4            10.0  \n",
      "Neighbourhoods\n",
      "   neighbourhood_group  neighbourhood\n",
      "0                  NaN      Amager st\n",
      "1                  NaN    Amager Vest\n",
      "2                  NaN     Bispebjerg\n",
      "3                  NaN   Brnshj-Husum\n",
      "4                  NaN  Frederiksberg\n"
     ]
    }
   ],
   "source": [
    "for name, df in datasets.items():\n",
    "    print(name)\n",
    "    print(df.head())\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d66d0be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§¹ CLEANING COMPLETELY NaN COLUMNS\n",
      "\n",
      "ï¿½ï¿½ï¸ Removing 2 completely NaN columns from Listings:\n",
      "   âŒ neighbourhood_group\n",
      "   âŒ license\n",
      "âœ… Listings shape after cleaning: (21722, 16)\n",
      "   ğŸ“ Removed 2 columns\n",
      "\n",
      "âœ… Reviews: No completely NaN columns found\n",
      "\n",
      "ï¿½ï¿½ï¸ Removing 1 completely NaN columns from Calendar:\n",
      "   âŒ adjusted_price\n",
      "âœ… Calendar shape after cleaning: (7928517, 6)\n",
      "   ğŸ“ Removed 1 columns\n",
      "\n",
      "ï¿½ï¿½ï¸ Removing 1 completely NaN columns from Neighbourhoods:\n",
      "   âŒ neighbourhood_group\n",
      "âœ… Neighbourhoods shape after cleaning: (11, 1)\n",
      "   ğŸ“ Removed 1 columns\n",
      "\n",
      "âœ… All datasets cleaned!\n"
     ]
    }
   ],
   "source": [
    "def remove_completely_nan_columns(df, name):\n",
    "    \"\"\"Remove columns that are completely NaN and report the changes\"\"\"\n",
    "    if df is None:\n",
    "        return df\n",
    "    \n",
    "    # Find columns that are completely NaN\n",
    "    completely_nan_cols = df.columns[df.isnull().all()].tolist()\n",
    "    \n",
    "    if completely_nan_cols:\n",
    "        print(f\"ï¿½ï¿½ï¸ Removing {len(completely_nan_cols)} completely NaN columns from {name}:\")\n",
    "        for col in completely_nan_cols:\n",
    "            print(f\"   âŒ {col}\")\n",
    "        \n",
    "        # Remove the columns\n",
    "        df_cleaned = df.drop(columns=completely_nan_cols)\n",
    "        \n",
    "        print(f\"âœ… {name} shape after cleaning: {df_cleaned.shape}\")\n",
    "        print(f\"   ğŸ“ Removed {len(completely_nan_cols)} columns\")\n",
    "        print()\n",
    "        \n",
    "        return df_cleaned\n",
    "    else:\n",
    "        print(f\"âœ… {name}: No completely NaN columns found\")\n",
    "        print()\n",
    "        return df\n",
    "\n",
    "# Clean all datasets\n",
    "print(\"ğŸ§¹ CLEANING COMPLETELY NaN COLUMNS\\n\")\n",
    "\n",
    "# Clean each dataset\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        datasets[name] = remove_completely_nan_columns(df, name)\n",
    "\n",
    "# Update the individual dataframe variables\n",
    "if 'Listings' in datasets:\n",
    "    listings_df = datasets['Listings']\n",
    "if 'Reviews' in datasets:\n",
    "    reviews_df = datasets['Reviews']\n",
    "if 'Calendar' in datasets:\n",
    "    calendar_df = datasets['Calendar']\n",
    "if 'Neighbourhoods' in datasets:\n",
    "    neighbourhoods_df = datasets['Neighbourhoods']\n",
    "\n",
    "print(\"âœ… All datasets cleaned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ca0b74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï¿½ï¿½ VERIFICATION AFTER CLEANING\n",
      "\n",
      "ï¿½ï¿½ Listings Dataset:\n",
      "   ğŸ“ Shape: (21722, 16)\n",
      "   ğŸ“‹ Columns: 16\n",
      "   âœ… No completely NaN columns remaining\n",
      "   ï¿½ï¿½ Memory: 8.84 MB\n",
      "\n",
      "ï¿½ï¿½ Reviews Dataset:\n",
      "   ğŸ“ Shape: (405687, 2)\n",
      "   ğŸ“‹ Columns: 2\n",
      "   âœ… No completely NaN columns remaining\n",
      "   ï¿½ï¿½ Memory: 6.19 MB\n",
      "\n",
      "ï¿½ï¿½ Calendar Dataset:\n",
      "   ğŸ“ Shape: (7928517, 6)\n",
      "   ğŸ“‹ Columns: 6\n",
      "   âœ… No completely NaN columns remaining\n",
      "   ï¿½ï¿½ Memory: 1051.76 MB\n",
      "\n",
      "ï¿½ï¿½ Neighbourhoods Dataset:\n",
      "   ğŸ“ Shape: (11, 1)\n",
      "   ğŸ“‹ Columns: 1\n",
      "   âœ… No completely NaN columns remaining\n",
      "   ï¿½ï¿½ Memory: 0.00 MB\n",
      "\n",
      "ğŸ¯ Ready for further analysis!\n"
     ]
    }
   ],
   "source": [
    "print(\"ï¿½ï¿½ VERIFICATION AFTER CLEANING\\n\")\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        print(f\"ï¿½ï¿½ {name} Dataset:\")\n",
    "        print(f\"   ğŸ“ Shape: {df.shape}\")\n",
    "        print(f\"   ğŸ“‹ Columns: {len(df.columns)}\")\n",
    "        \n",
    "        # Check for any remaining completely NaN columns\n",
    "        completely_nan_cols = df.columns[df.isnull().all()].tolist()\n",
    "        if completely_nan_cols:\n",
    "            print(f\"   âš ï¸ Still has {len(completely_nan_cols)} completely NaN columns\")\n",
    "        else:\n",
    "            print(f\"   âœ… No completely NaN columns remaining\")\n",
    "        \n",
    "        # Show memory usage\n",
    "        memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        print(f\"   ï¿½ï¿½ Memory: {memory_mb:.2f} MB\")\n",
    "        print()\n",
    "    else:\n",
    "        print(f\"âŒ {name} Dataset: Not available\\n\")\n",
    "\n",
    "print(\"ğŸ¯ Ready for further analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45d00285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” CHECKING FOR LISTING ID COLUMNS\n",
      "\n",
      "âœ… Listings: Found ID columns - ['id']\n",
      "âœ… Reviews: Found ID columns - ['listing_id']\n",
      "âœ… Calendar: Found ID columns - ['listing_id']\n",
      "âŒ Neighbourhoods: No listing ID columns found\n",
      "\n",
      "ğŸ“Š Datasets with ID columns: ['Listings', 'Reviews', 'Calendar']\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ” CHECKING FOR LISTING ID COLUMNS\\n\")\n",
    "\n",
    "# Check which datasets have listing_id columns\n",
    "listing_id_cols = {}\n",
    "for name, df in datasets.items():\n",
    "    if df is not None:\n",
    "        # Look for listing_id columns (case insensitive)\n",
    "        id_cols = [col for col in df.columns if 'listing_id' in col.lower() or col.lower() == 'id']\n",
    "        if id_cols:\n",
    "            listing_id_cols[name] = id_cols\n",
    "            print(f\"âœ… {name}: Found ID columns - {id_cols}\")\n",
    "        else:\n",
    "            print(f\"âŒ {name}: No listing ID columns found\")\n",
    "    else:\n",
    "        print(f\"âŒ {name}: Dataset not available\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Datasets with ID columns: {list(listing_id_cols.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a132c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— ANALYZING LISTING ID RELATIONSHIPS\n",
      "\n",
      "ğŸ“Š Listings dataset has 21722 unique listing IDs\n",
      "â­ Reviews dataset has 18758 unique listing IDs\n",
      "   ğŸ”— Common with Listings: 18758 (86.4%)\n",
      "ğŸ“… Calendar dataset has 21722 unique listing IDs\n",
      "   ğŸ”— Common with Listings: 21722 (100.0%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”— ANALYZING LISTING ID RELATIONSHIPS\\n\")\n",
    "\n",
    "# Check for common listing IDs between datasets\n",
    "if 'Listings' in listing_id_cols and listings_df is not None:\n",
    "    listings_ids = set(listings_df[listing_id_cols['Listings'][0]].dropna())\n",
    "    print(f\"ğŸ“Š Listings dataset has {len(listings_ids)} unique listing IDs\")\n",
    "    \n",
    "    # Check Reviews dataset\n",
    "    if 'Reviews' in listing_id_cols and reviews_df is not None:\n",
    "        reviews_ids = set(reviews_df[listing_id_cols['Reviews'][0]].dropna())\n",
    "        common_reviews = listings_ids.intersection(reviews_ids)\n",
    "        print(f\"â­ Reviews dataset has {len(reviews_ids)} unique listing IDs\")\n",
    "        print(f\"   ğŸ”— Common with Listings: {len(common_reviews)} ({len(common_reviews)/len(listings_ids)*100:.1f}%)\")\n",
    "    \n",
    "    # Check Calendar dataset\n",
    "    if 'Calendar' in listing_id_cols and calendar_df is not None:\n",
    "        calendar_ids = set(calendar_df[listing_id_cols['Calendar'][0]].dropna())\n",
    "        common_calendar = listings_ids.intersection(calendar_ids)\n",
    "        print(f\"ğŸ“… Calendar dataset has {len(calendar_ids)} unique listing IDs\")\n",
    "        print(f\"   ğŸ”— Common with Listings: {len(common_calendar)} ({len(common_calendar)/len(listings_ids)*100:.1f}%)\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c142ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”— JOINING DATASETS ON LISTING ID (FIXED)\n",
      "\n",
      "ï¿½ï¿½ Base dataset: Listings ((21722, 16))\n",
      "   ï¿½ï¿½ Using ID column: id\n",
      "\n",
      "ğŸ”„ Joining with Reviews dataset...\n",
      "   ï¿½ï¿½ Available columns: ['listing_id', 'date']\n",
      "   âœ… Joined Reviews summary: (21722, 19)\n",
      "   ğŸ“Š Reviews columns added: ['total_reviews', 'latest_date', 'earliest_date']\n",
      "\n",
      "ğŸ”„ Joining with Calendar dataset...\n",
      "   âœ… Joined Calendar summary: (21722, 23)\n",
      "   ğŸ“Š Calendar columns added: ['available_days', 'total_calendar_days', 'availability_rate', 'avg_price']\n",
      "\n",
      "ğŸ”„ Adding neighbourhood information...\n",
      "   âœ… Joined Neighbourhoods: (21722, 23)\n",
      "\n",
      "ğŸ¯ Final joined dataset shape: (21722, 23)\n",
      "ğŸ“‹ Total columns: 23\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ”— JOINING DATASETS ON LISTING ID (FIXED)\\n\")\n",
    "\n",
    "# Start with listings as the base dataset\n",
    "if listings_df is not None and 'Listings' in listing_id_cols:\n",
    "    base_df = listings_df.copy()\n",
    "    listings_id_col = listing_id_cols['Listings'][0]\n",
    "    print(f\"ï¿½ï¿½ Base dataset: Listings ({base_df.shape})\")\n",
    "    print(f\"   ï¿½ï¿½ Using ID column: {listings_id_col}\")\n",
    "    \n",
    "    # Join with Reviews\n",
    "    if 'Reviews' in listing_id_cols and reviews_df is not None:\n",
    "        reviews_id_col = listing_id_cols['Reviews'][0]\n",
    "        print(f\"\\nğŸ”„ Joining with Reviews dataset...\")\n",
    "        \n",
    "        # Get actual columns from reviews dataset\n",
    "        reviews_actual_cols = reviews_df.columns.tolist()\n",
    "        print(f\"   ï¿½ï¿½ Available columns: {reviews_actual_cols}\")\n",
    "        \n",
    "        # Create reviews summary based on available columns\n",
    "        reviews_summary = reviews_df.groupby(reviews_id_col).agg({\n",
    "            reviews_id_col: 'count'  # Count of reviews per listing\n",
    "        }).rename(columns={reviews_id_col: 'total_reviews'})\n",
    "        \n",
    "        # Add rating columns if they exist\n",
    "        rating_cols = [col for col in reviews_df.columns if 'rating' in col.lower() or 'score' in col.lower()]\n",
    "        for col in rating_cols:\n",
    "            reviews_summary[f'avg_{col}'] = reviews_df.groupby(reviews_id_col)[col].mean()\n",
    "        \n",
    "        # Add date columns if they exist\n",
    "        date_cols = [col for col in reviews_df.columns if 'date' in col.lower()]\n",
    "        for col in date_cols:\n",
    "            reviews_summary[f'latest_{col}'] = reviews_df.groupby(reviews_id_col)[col].max()\n",
    "            reviews_summary[f'earliest_{col}'] = reviews_df.groupby(reviews_id_col)[col].min()\n",
    "        \n",
    "        # Add comment length if comments column exists\n",
    "        comment_cols = [col for col in reviews_df.columns if 'comment' in col.lower()]\n",
    "        for col in comment_cols:\n",
    "            reviews_summary[f'avg_{col}_length'] = reviews_df.groupby(reviews_id_col)[col].apply(\n",
    "                lambda x: x.str.len().mean() if x.dtype == 'object' else 0\n",
    "            )\n",
    "        \n",
    "        # Join with base dataset\n",
    "        base_df = base_df.merge(\n",
    "            reviews_summary, \n",
    "            left_on=listings_id_col, \n",
    "            right_index=True, \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Joined Reviews summary: {base_df.shape}\")\n",
    "        print(f\"   ğŸ“Š Reviews columns added: {list(reviews_summary.columns)}\")\n",
    "    \n",
    "    # Join with Calendar\n",
    "    if 'Calendar' in listing_id_cols and calendar_df is not None:\n",
    "        calendar_id_col = listing_id_cols['Calendar'][0]\n",
    "        print(f\"\\nğŸ”„ Joining with Calendar dataset...\")\n",
    "        \n",
    "        # Create calendar summary (aggregate calendar data per listing)\n",
    "        calendar_summary = calendar_df.groupby(calendar_id_col).agg({\n",
    "            'available': lambda x: (x == 't').sum() if 'available' in calendar_df.columns else 0,  # Available days\n",
    "        }).rename(columns={'available': 'available_days'})\n",
    "        \n",
    "        # Add total calendar days\n",
    "        calendar_summary['total_calendar_days'] = calendar_df.groupby(calendar_id_col).size()\n",
    "        calendar_summary['availability_rate'] = calendar_summary['available_days'] / calendar_summary['total_calendar_days']\n",
    "        \n",
    "        # Add price analysis if price column exists\n",
    "        if 'price' in calendar_df.columns:\n",
    "            calendar_summary['avg_price'] = calendar_df.groupby(calendar_id_col)['price'].apply(\n",
    "                lambda x: pd.to_numeric(x.str.replace('$', '').str.replace(',', ''), errors='coerce').mean()\n",
    "            )\n",
    "        \n",
    "        # Join with base dataset\n",
    "        base_df = base_df.merge(\n",
    "            calendar_summary, \n",
    "            left_on=listings_id_col, \n",
    "            right_index=True, \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Joined Calendar summary: {base_df.shape}\")\n",
    "        print(f\"   ğŸ“Š Calendar columns added: {list(calendar_summary.columns)}\")\n",
    "    \n",
    "    # Join with Neighbourhoods (if it has neighbourhood info)\n",
    "    if neighbourhoods_df is not None:\n",
    "        print(f\"\\nğŸ”„ Adding neighbourhood information...\")\n",
    "        \n",
    "        # Check if listings has neighbourhood column\n",
    "        neighbourhood_cols = [col for col in base_df.columns if 'neighbourhood' in col.lower()]\n",
    "        if neighbourhood_cols:\n",
    "            neighbourhood_col = neighbourhood_cols[0]\n",
    "            base_df = base_df.merge(\n",
    "                neighbourhoods_df,\n",
    "                left_on=neighbourhood_col,\n",
    "                right_on='neighbourhood',\n",
    "                how='left'\n",
    "            )\n",
    "            print(f\"   âœ… Joined Neighbourhoods: {base_df.shape}\")\n",
    "    \n",
    "    print(f\"\\nğŸ¯ Final joined dataset shape: {base_df.shape}\")\n",
    "    print(f\"ğŸ“‹ Total columns: {len(base_df.columns)}\")\n",
    "    \n",
    "    # Store the joined dataset\n",
    "    joined_df = base_df\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ Cannot join datasets - Listings dataset or ID column not available\")\n",
    "    joined_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9f02290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š JOINED DATASET OVERVIEW\n",
      "\n",
      "ğŸ“ Shape: (21722, 23)\n",
      "ğŸ“‹ Columns: 23\n",
      "ï¿½ï¿½ Memory: 10.00 MB\n",
      "\n",
      "ğŸ†• New columns from joins (7):\n",
      "   â€¢ total_reviews                  (Missing:  13.6%)\n",
      "   â€¢ latest_date                    (Missing:  13.6%)\n",
      "   â€¢ earliest_date                  (Missing:  13.6%)\n",
      "   â€¢ available_days                 (Missing:   0.0%)\n",
      "   â€¢ total_calendar_days            (Missing:   0.0%)\n",
      "   â€¢ availability_rate              (Missing:   0.0%)\n",
      "   â€¢ avg_price                      (Missing:   0.0%)\n",
      "\n",
      "ğŸ‘€ Sample of joined data:\n",
      "      id                                               name  host_id  \\\n",
      "0  31094  Beautiful, spacious, central, renovated Penthouse   129976   \n",
      "1  32379  155 m2 artist flat on Vesterbro, with 2 bathrooms   140105   \n",
      "2  32841                Cozy flat for Adults/Quiet for kids   142143   \n",
      "\n",
      "        host_name              neighbourhood   latitude  longitude  \\\n",
      "0            Ebbe  Vesterbro-Kongens Enghave  55.666602  12.555283   \n",
      "1            Lise  Vesterbro-Kongens Enghave  55.672638  12.552493   \n",
      "2  Anders & Maria                    sterbro  55.711760  12.570910   \n",
      "\n",
      "         room_type  price  minimum_nights  ...  \\\n",
      "0  Entire home/apt    NaN               3  ...   \n",
      "1  Entire home/apt    NaN               3  ...   \n",
      "2  Entire home/apt    NaN             100  ...   \n",
      "\n",
      "   calculated_host_listings_count availability_365  number_of_reviews_ltm  \\\n",
      "0                               1                0                      0   \n",
      "1                               2                0                      3   \n",
      "2                               1               15                      0   \n",
      "\n",
      "   total_reviews  latest_date  earliest_date  available_days  \\\n",
      "0           19.0   2022-08-22     2010-08-16               0   \n",
      "1           84.0   2024-10-28     2010-08-23               0   \n",
      "2            7.0   2016-09-15     2010-07-25              15   \n",
      "\n",
      "  total_calendar_days availability_rate  avg_price  \n",
      "0                 365          0.000000     2299.0  \n",
      "1                 365          0.000000      250.0  \n",
      "2                 365          0.041096       79.0  \n",
      "\n",
      "[3 rows x 23 columns]\n",
      "\n",
      "ğŸ“ Data types:\n",
      "int64             9\n",
      "float64           7\n",
      "object            5\n",
      "datetime64[ns]    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if joined_df is not None:\n",
    "    print(\"ğŸ“Š JOINED DATASET OVERVIEW\\n\")\n",
    "    \n",
    "    print(f\"ğŸ“ Shape: {joined_df.shape}\")\n",
    "    print(f\"ğŸ“‹ Columns: {len(joined_df.columns)}\")\n",
    "    print(f\"ï¿½ï¿½ Memory: {joined_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Show new columns from joins\n",
    "    original_listings_cols = set(listings_df.columns)\n",
    "    new_cols = [col for col in joined_df.columns if col not in original_listings_cols]\n",
    "    \n",
    "    if new_cols:\n",
    "        print(f\"\\nğŸ†• New columns from joins ({len(new_cols)}):\")\n",
    "        for col in new_cols:\n",
    "            missing_pct = (joined_df[col].isnull().sum() / len(joined_df)) * 100\n",
    "            print(f\"   â€¢ {col:<30} (Missing: {missing_pct:5.1f}%)\")\n",
    "    \n",
    "    # Show sample of joined data\n",
    "    print(f\"\\nğŸ‘€ Sample of joined data:\")\n",
    "    print(joined_df.head(3))\n",
    "    \n",
    "    # Show data types\n",
    "    print(f\"\\nğŸ“ Data types:\")\n",
    "    print(joined_df.dtypes.value_counts())\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No joined dataset available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
